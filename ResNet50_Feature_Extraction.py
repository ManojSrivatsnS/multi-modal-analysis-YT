import torch
import torch.nn as nn
from torchvision import models
from torchvision.models import ResNet50_Weights # For pretrained weights
from tqdm import tqdm
import numpy as np
from pathlib import Path
import pandas as pd
import re # Not strictly needed for this version, but kept for consistency

# === CONFIGURATION ===
# Directory containing your raw .pt tensor files (frames extracted from videos)
TENSOR_DIR = Path(r"C:\Users\manoj\Downloads\University UK docs\University UK docs\classes\PRoject dissertation\downloaded_videos\ind_sets_tensors\data\Ind_Train_Tensors")
# Directory where the extracted ResNet50 features will be saved
FEATURE_DIR = Path(r"C:\Users\manoj\Downloads\University UK docs\University UK docs\classes\PRoject dissertation\downloaded_videos\ind_sets_tensors\features\Ind_Train_ResNet50")
FEATURE_DIR.mkdir(parents=True, exist_ok=True) # Ensure this directory exists

# Path to the prepared metadata Excel file generated by Script 1
PREPARED_METADATA_PATH = Path(r"C:\Users\manoj\Downloads\University UK docs\University UK docs\classes\PRoject dissertation\downloaded_videos\ind_sets_tensors\features\prepared_for_feature_extraction.xlsx")
# Output path for the final updated metadata after feature extraction
FINAL_METADATA_PATH = Path(r"C:\Users\manoj\Downloads\University UK docs\University UK docs\classes\PRoject dissertation\downloaded_videos\ind_sets_tensors\features\final_metadata_with_features_status.xlsx")

# Control how many matched files to process (set to None to process all matched)
# Set to an integer like 100, 50, or None for all.
# This processes files that are 'Found' in the metadata and *not yet* extracted.
NUM_FILES_TO_PROCESS = 581 # <--- Adjust this value (e.g., 100) or leave as None for all

# === SETUP DEVICE ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on:", device)

# === LOAD RESNET50 BACKBONE ===
# Use ResNet50_Weights.DEFAULT for recommended pretrained weights
resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)
resnet50 = nn.Sequential(*list(resnet50.children())[:-2]) # Remove avgpool + fc layers
resnet50.eval().to(device) # Set to evaluation mode and move to device

# === MAIN SCRIPT LOGIC ===

print("üöÄ Starting ResNet50 feature extraction process...")

# --- 1. Load Prepared Metadata ---
try:
    # Load the 'Metadata_For_Extraction' sheet from the prepared file
    df_features_status = pd.read_excel(PREPARED_METADATA_PATH, sheet_name='Metadata_For_Extraction')
    print(f"üìñ Loaded prepared metadata from: {PREPARED_METADATA_PATH} (Sheet: Metadata_For_Extraction)")
except Exception as e:
    print(f"‚ùå Error loading prepared metadata from {PREPARED_METADATA_PATH} or 'Metadata_For_Extraction' sheet: {e}")
    print("Please ensure Script 1 was run successfully and the sheet exists.")
    exit()

# Ensure 'Used_In_ResNet50' column exists and initialize if not
if 'Used_In_ResNet50' not in df_features_status.columns:
    df_features_status['Used_In_ResNet50'] = "No" # Default for all rows

# Filter for entries where a tensor file was found by Script 1
# And where features have NOT yet been extracted (Used_In_ResNet50 is 'No')
df_to_process = df_features_status[
    (df_features_status['Tensor_Found_Status'] == 'Found') &
    (df_features_status['Matched_Tensor_Filename'].notna()) &
    (df_features_status['Matched_Tensor_Filename'] != '') &
    (df_features_status['Used_In_ResNet50'] == 'No') # Only process files not yet processed
].copy() # Use .copy() to avoid SettingWithCopyWarning

print(f"‚úÖ Found {len(df_to_process)} metadata entries with matched tensor files that need processing.")

# Apply NUM_FILES_TO_PROCESS limit if set
if NUM_FILES_TO_PROCESS is not None and NUM_FILES_TO_PROCESS > 0:
    df_to_process = df_to_process.head(NUM_FILES_TO_PROCESS)
    print(f"‚úÖ Limiting processing to the first {len(df_to_process)} *unprocessed* files.")

# --- 2. Feature Extraction Loop ---
processed_this_run_count = 0
for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc="Extracting features"):
    serial_number = row['serial number']
    matched_tensor_filename = row['Matched_Tensor_Filename']

    input_tensor_path = TENSOR_DIR / matched_tensor_filename
    # Output feature file will be named by serial number (e.g., '629.pt')
    output_feature_path = FEATURE_DIR / f"{serial_number}.pt"

    # Check if the output feature file already exists.
    # This avoids reprocessing and ensures no duplicates are created based on serial number.
    if output_feature_path.exists():
        print(f"‚è© Skipping serial {serial_number}: Feature file already exists at {output_feature_path}.")
        # Update status in the main df_features_status DataFrame if it's not already 'Yes'
        if df_features_status.loc[df_features_status['serial number'] == serial_number, 'Used_In_ResNet50'].iloc[0] == "No":
            df_features_status.loc[df_features_status['serial number'] == serial_number, 'Used_In_ResNet50'] = "Yes"
        continue # Move to next file

    try:
        # Load the raw tensor (frames)
        tensor_frames = torch.load(input_tensor_path)
        
        # Ensure tensor is in (N, C, H, W) format for ResNet50 (N, 3, 128, 128)
        # Assuming input tensors are (960, 128, 128, 3) from your previous code
        if tensor_frames.dim() == 4 and tensor_frames.shape[3] == 3: # (N, H, W, C)
            tensor_frames = tensor_frames.permute(0, 3, 1, 2) # Convert to (N, C, H, W)
        
        tensor_frames = tensor_frames.to(device)

        # Extract features
        with torch.no_grad():
            features = resnet50(tensor_frames) # (N, 2048, H', W') e.g., (960, 2048, 4, 4)
            pooled = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1)) # (N, 2048, 1, 1)
            pooled = pooled.view(pooled.size(0), -1).cpu() # (N, 2048) and move to CPU

        # Save extracted feature tensor with serial number as filename
        torch.save(pooled, output_feature_path)
        print(f"‚úÖ Features extracted and saved for serial {serial_number} to {output_feature_path}")
        processed_this_run_count += 1

        # Update status in the main df_features_status DataFrame
        df_features_status.loc[df_features_status['serial number'] == serial_number, 'Used_In_ResNet50'] = "Yes"

    except FileNotFoundError:
        print(f"‚ùå Input tensor file not found for serial {serial_number} at {input_tensor_path}. Skipping.")
        # Mark as 'No' if input file not found (should ideally be caught by Script 1)
        df_features_status.loc[df_features_status['serial number'] == serial_number, 'Used_In_ResNet50'] = "No"
    except Exception as e:
        print(f"‚ùå Failed to process serial {serial_number} from {input_tensor_path}: {e}. Skipping.")
        # Mark as 'No' if processing failed
        df_features_status.loc[df_features_status['serial number'] == serial_number, 'Used_In_ResNet50'] = "No"

# --- 3. Save Final Updated Metadata ---
df_features_status.to_excel(FINAL_METADATA_PATH, index=False)
print(f"\n‚úÖ Final updated metadata saved to:\n{FINAL_METADATA_PATH}")
print(f"‚úÖ Extracted features saved in:\n{FEATURE_DIR}")
print(f"üéâ Feature extraction complete. Processed {processed_this_run_count} new files in this run.")
print(f"Total files marked 'Used_In_ResNet50' as 'Yes' in final metadata: {df_features_status['Used_In_ResNet50'].value_counts().get('Yes', 0)}")
try:
    # Attempt to save the DataFrame to an Excel file
    df_features_status.to_excel(FINAL_METADATA_PATH, index=False)
    print(f"‚úÖ Final metadata saved successfully to {FINAL_METADATA_PATH}")
except Exception as e:
    print(f"‚ùå Error saving final metadata to {FINAL_METADATA_PATH}: {e}")
    print("Please ensure the file is not open and you have 'xlsxwriter' installed ('pip install xlsxwriter').") 
# === DONE ===
# This script extracts ResNet50 features from tensors, saves them, and updates metadata to indicate
# which videos were processed. It handles existing feature files to avoid duplicates and ensures
# robust error handling for missing files or processing issues.